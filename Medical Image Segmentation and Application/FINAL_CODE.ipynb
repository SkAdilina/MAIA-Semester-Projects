{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahDkQz1edLMp"
   },
   "source": [
    "MISA 3D Brain MRI Segmentation using 2D UNet\n",
    "\n",
    "1. Preprocessing - Bias Correction\n",
    "\n",
    "2. Method - Patch Based\n",
    "\n",
    "3. Data Augmentation - Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMIG5gOwa-Nb"
   },
   "source": [
    "# Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "spA2vbr3a0j7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt # plotting purposes\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import glob\n",
    "import cv2\n",
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "import SimpleITK as sitk\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEFUu8FjYvaz"
   },
   "source": [
    "# Defining the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "\n",
    "###################################\n",
    "#           Tversky loss          #\n",
    "###################################\n",
    "\n",
    "def TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(inputs)\n",
    "        targets = K.flatten(targets)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "       \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        \n",
    "        return 1 - Tversky\n",
    "    \n",
    "#Keras\n",
    "ALPHA = 0.5\n",
    "BETA = 0.5\n",
    "GAMMA = 1\n",
    "\n",
    "########################################\n",
    "#           FocalTversky loss          #\n",
    "########################################\n",
    "\n",
    "def FocalTverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, gamma=GAMMA, smooth=1e-6):\n",
    "    \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = K.flatten(inputs)\n",
    "        targets = K.flatten(targets)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = K.sum((inputs * targets))\n",
    "        FP = K.sum(((1-targets) * inputs))\n",
    "        FN = K.sum((targets * (1-inputs)))\n",
    "               \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = K.pow((1 - Tversky), gamma)\n",
    "        \n",
    "        return FocalTversky\n",
    "    \n",
    "    \n",
    "#Keras\n",
    "ALPHA = 0.8\n",
    "GAMMA = 2\n",
    "\n",
    "#################################\n",
    "#           Focal loss          #\n",
    "#################################\n",
    "\n",
    "def FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n",
    "    \n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "    \n",
    "    BCE = K.binary_crossentropy(targets, inputs)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "\n",
    "################################\n",
    "#           Dice loss          #\n",
    "################################\n",
    "\n",
    "#Keras\n",
    "# Helper function to enable loss function to be flexibly used for \n",
    "# both 2D or 3D image segmentation - source: https://github.com/frankkramer-lab/MIScnn\n",
    "def identify_axis(shape):\n",
    "    # Three dimensional\n",
    "    if len(shape) == 5 : return [1,2,3]\n",
    "    # Two dimensional\n",
    "    elif len(shape) == 4 : return [1,2]\n",
    "    # Exception - Unknown\n",
    "    else : raise ValueError('Metric: Shape of tensor is neither 2D or 3D.')\n",
    "\n",
    "\n",
    "def DiceLoss(y_true, y_pred, delta = 0.5, smooth = 0.000001):\n",
    "    axis = identify_axis(y_true.get_shape())\n",
    "    # Calculate true positives (tp), false negatives (fn) and false positives (fp)\n",
    "    tp = K.sum(y_true * y_pred, axis=axis)\n",
    "    fn = K.sum(y_true * (1-y_pred), axis=axis)\n",
    "    fp = K.sum((1-y_true) * y_pred, axis=axis)\n",
    "    # Calculate Dice score\n",
    "    dice_class = (tp + smooth)/(tp + delta*fn + (1-delta)*fp + smooth)\n",
    "    # Average class scores\n",
    "    dice_loss = K.mean(1-dice_class)\n",
    "\n",
    "    return dice_loss\n",
    "\n",
    "###############################\n",
    "#          Combo loss         #\n",
    "###############################\n",
    "\n",
    "\n",
    "def ComboLoss(y_true, y_pred, alpha=0.5, beta=0.5):\n",
    "    \n",
    "    dice = DiceLoss(y_true, y_pred)\n",
    "    axis = identify_axis(y_true.get_shape())\n",
    "    # Clip values to prevent division by zero error\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -y_true * K.log(y_pred)\n",
    "\n",
    "    if beta is not None:\n",
    "        beta_weight = np.array([beta, 1-beta])\n",
    "        cross_entropy = beta_weight * cross_entropy\n",
    "    # sum over classes\n",
    "    cross_entropy = K.mean(K.sum(cross_entropy, axis=[-1]))\n",
    "    if alpha is not None:\n",
    "        combo_loss = (alpha * cross_entropy) - ((1 - alpha) * dice)\n",
    "    else:\n",
    "        combo_loss = cross_entropy - dice\n",
    "    return combo_loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eerKWEtxUg_x"
   },
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "IMAGE_SIZE = (256, 128, 256)\n",
    "\n",
    "# training, validation, test parameters\n",
    "TRAINING_VOLUMES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "VALIDATION_VOLUMES = [9]\n",
    "\n",
    "# network parameters\n",
    "N_CLASSES = 4\n",
    "N_INPUT_CHANNELS = 1\n",
    "PATCH_SIZE = (32, 32)\n",
    "PATCH_STRIDE = (32, 32)\n",
    "\n",
    "# data preparation parameters\n",
    "CONTENT_THRESHOLD = 0.3 # to get rid of useless info in the image\n",
    "\n",
    "# training parameters\n",
    "N_EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 20\n",
    "MODEL_FNAME_PATTERN = 'model-200-categorical_crossentropy.h5'\n",
    "OPTIMISER = 'Adam'\n",
    "LOSS = 'categorical_crossentropy'\n",
    "dropout_rate = 0.40\n",
    "\n",
    "list_id = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYqbV-aZEzEx"
   },
   "source": [
    "**Mount drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18449,
     "status": "ok",
     "timestamp": 1641467746539,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "tFJ_L_2i55hx",
    "outputId": "cfd5e9bb-bb95-4b50-943b-d206822a4cdb"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive._mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kakQO5X9E0oR"
   },
   "source": [
    "# Define UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "flPaOqv0bZ3-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# new-model\\ndef get_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\\n    \\n    inputs = keras.Input(shape=img_size + (n_input_channels, ))\\n\\n    # Encoding Path of the UNet (32-64-128-256-512)\\n    conv1   = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(inputs)\\n    drop1   = layers.Dropout(rate=dropout_rate)(conv1, training=True)\\n    max1    = layers.MaxPooling2D((2, 2))(drop1)\\n\\n    conv2   = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(max1)\\n    drop2   = layers.Dropout(rate=dropout_rate)(conv2, training=True)\\n    max2    = layers.MaxPooling2D((2, 2))(drop2)\\n\\n    conv3   = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(max2)\\n    drop3   = layers.Dropout(rate=dropout_rate)(conv3, training=True)\\n    max3    = layers.MaxPooling2D((2, 2))(drop3)\\n\\n    conv4   = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(max3)\\n    drop4   = layers.Dropout(rate=dropout_rate)(conv4, training=True)\\n    max4    = layers.MaxPooling2D((2, 2))(drop4)\\n\\n    lat     = layers.Conv2D(512*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(max4)\\n    drop5   = layers.Dropout(rate=dropout_rate)(lat, training=True)\\n\\n    # Decoding Path of the UNet\\n    up1     = layers.UpSampling2D((2, 2))(drop5)\\n    concat1 = layers.concatenate([conv4, up1], axis=-1)\\n    conv5   = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(concat1)\\n    drop6   = layers.Dropout(rate=dropout_rate)(conv5, training=True)\\n    \\n    up2     = layers.UpSampling2D((2, 2))(drop6)\\n    concat2 = layers.concatenate([conv3, up2], axis=-1)\\n    conv6   = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(concat2)\\n    drop7   = layers.Dropout(rate=dropout_rate)(conv6, training=True)\\n    \\n    up3     = layers.UpSampling2D((2, 2))(drop7)\\n    concat3 = layers.concatenate([conv2, up3], axis=-1)\\n    conv7   = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(concat3)\\n    drop8   = layers.Dropout(rate=dropout_rate)(conv7, training=True)\\n\\n    up4     = layers.UpSampling2D((2, 2))(drop8)\\n    concat4 = layers.concatenate([conv1, up4], axis=-1)\\n    conv8   = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation=\\'relu\\')(concat4)\\n    drop9   = layers.Dropout(rate=dropout_rate)(conv8, training=True)\\n    \\n    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(drop9)\\n\\n    model   = keras.Model(inputs, outputs)\\n\\n    return model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "def get_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
    "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
    "\n",
    "    # Encoding path\n",
    "    conv1 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
    "    drop1 = layers.Dropout(rate=dropout_rate)(conv1, training=True)\n",
    "    max1 = layers.MaxPooling2D((2, 2))(drop1)\n",
    "    # max1 = layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1)\n",
    "    drop2 = layers.Dropout(rate=dropout_rate)(conv2, training=True)\n",
    "    # max2 = layers.MaxPooling2D((2, 2))(conv2)\n",
    "    max2 = layers.MaxPooling2D((2, 2))(drop2)\n",
    "\n",
    "    conv3 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2)\n",
    "    drop3 = layers.Dropout(rate=dropout_rate)(conv3, training=True)\n",
    "    # max3 = layers.MaxPooling2D((2, 2))(conv3)\n",
    "    max3 = layers.MaxPooling2D((2, 2))(drop3)\n",
    "\n",
    "    lat = layers.Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3)\n",
    "    drop4 = layers.Dropout(rate=dropout_rate)(lat, training=True)\n",
    "\n",
    "    # Decoding path\n",
    "    #up1 = layers.UpSampling2D((2, 2))(lat)\n",
    "    up1 = layers.UpSampling2D((2, 2))(drop4)\n",
    "    concat1 = layers.concatenate([conv3, up1], axis=-1)\n",
    "    conv4 = layers.Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat1)\n",
    "    drop5 = layers.Dropout(rate=dropout_rate)(conv4, training=True)\n",
    "    \n",
    "    #up2 = layers.UpSampling2D((2, 2))(conv4)\n",
    "    up2 = layers.UpSampling2D((2, 2))(drop5)\n",
    "    concat2 = layers.concatenate([conv2, up2], axis=-1)\n",
    "    conv5 = layers.Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat2)\n",
    "    drop6 = layers.Dropout(rate=dropout_rate)(conv5, training=True)\n",
    "    \n",
    "    #up3 = layers.UpSampling2D((2, 2))(conv5)\n",
    "    up3 = layers.UpSampling2D((2, 2))(drop6)\n",
    "    concat3 = layers.concatenate([conv1, up3], axis=-1)\n",
    "    conv6 = layers.Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat3)\n",
    "    drop7 = layers.Dropout(rate=dropout_rate)(conv6, training=True)\n",
    "    \n",
    "    #outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(conv6)\n",
    "    outputs = layers.Conv2D(n_classes, (1, 1), activation=\"softmax\")(drop7)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_res_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
    "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
    "\n",
    "    # Encoding Path of the ResUNet (32-64-128-256-512)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conc1 = concatenate([inputs, conv1], axis=3)\n",
    "    drop1 = Dropout(rate=dropout_rate)(conc1, training=True)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conc2 = concatenate([pool1, conv2], axis=3)\n",
    "    drop2 = Dropout(rate=dropout_rate)(conc2, training=True)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conc3 = concatenate([pool2, conv3], axis=3)\n",
    "    drop3 = Dropout(rate=dropout_rate)(conc3, training=True)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conc4 = concatenate([pool3, conv4], axis=3)\n",
    "    drop4 = Dropout(rate=dropout_rate)(conc4, training=True)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conc5 = concatenate([pool4, conv5], axis=3)\n",
    "    drop5 = Dropout(rate=dropout_rate)(conc5, training=True)\n",
    "\n",
    "    # Decoding Path of the ResUNet\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(drop5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conc6 = concatenate([up6, conv6], axis=3)\n",
    "    drop6 = Dropout(rate=dropout_rate)(conc6, training=True)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(drop6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conc7 = concatenate([up7, conv7], axis=3)\n",
    "    drop7 = Dropout(rate=dropout_rate)(conc7, training=True)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(drop7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conc8 = concatenate([up8, conv8], axis=3)\n",
    "    drop8 = Dropout(rate=dropout_rate)(conc8, training=True)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(drop8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conc9 = concatenate([up9, conv9], axis=3)\n",
    "    drop9 = Dropout(rate=dropout_rate)(conc9, training=True)\n",
    "\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(drop9)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def get_dense_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
    "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
    "\n",
    "    # Encoding Path of the DenseUNet (32-64-128-256-512)\n",
    "    conv11 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conc11 = concatenate([inputs, conv11], axis=3)\n",
    "    conv12 = Conv2D(32, (3, 3), activation='relu', padding='same')(conc11)\n",
    "    conc12 = concatenate([inputs, conv12], axis=3)\n",
    "    drop1 = Dropout(rate=dropout_rate)(conc12, training=True)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n",
    "\n",
    "    conv21 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conc21 = concatenate([pool1, conv21], axis=3)\n",
    "    conv22 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc21)\n",
    "    conc22 = concatenate([pool1, conv22], axis=3)\n",
    "    drop2 = Dropout(rate=dropout_rate)(conc22, training=True)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n",
    "\n",
    "    conv31 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conc31 = concatenate([pool2, conv31], axis=3)\n",
    "    conv32 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc31)\n",
    "    conc32 = concatenate([pool2, conv32], axis=3)\n",
    "    drop3 = Dropout(rate=dropout_rate)(conc32, training=True)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)\n",
    "\n",
    "    conv41 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conc41 = concatenate([pool3, conv41], axis=3)\n",
    "    conv42 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc41)\n",
    "    conc42 = concatenate([pool3, conv42], axis=3)\n",
    "    drop4 = Dropout(rate=dropout_rate)(conc42, training=True)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv51 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conc51 = concatenate([pool4, conv51], axis=3)\n",
    "    conv52 = Conv2D(512, (3, 3), activation='relu', padding='same')(conc51)\n",
    "    conc52 = concatenate([pool4, conv52], axis=3)\n",
    "    drop5 = Dropout(rate=dropout_rate)(conc52, training=True)\n",
    "\n",
    "    # Decoding Path of the ResUNet\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(drop5), conc42], axis=3)\n",
    "    conv61 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conc61 = concatenate([up6, conv61], axis=3)\n",
    "    conv62 = Conv2D(256, (3, 3), activation='relu', padding='same')(conc61)\n",
    "    conc62 = concatenate([up6, conv62], axis=3)\n",
    "    drop6 = Dropout(rate=dropout_rate)(conc62, training=True)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(drop6), conv32], axis=3)\n",
    "    conv71 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conc71 = concatenate([up7, conv71], axis=3)\n",
    "    conv72 = Conv2D(128, (3, 3), activation='relu', padding='same')(conc71)\n",
    "    conc72 = concatenate([up7, conv72], axis=3)\n",
    "    drop7 = Dropout(rate=dropout_rate)(conc72, training=True)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(drop7), conv22], axis=3)\n",
    "    conv81 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conc81 = concatenate([up8, conv81], axis=3)\n",
    "    conv82 = Conv2D(64, (3, 3), activation='relu', padding='same')(conc81)\n",
    "    conc82 = concatenate([up8, conv82], axis=3)\n",
    "    drop8 = Dropout(rate=dropout_rate)(conc82, training=True)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(drop8), conv12], axis=3)\n",
    "    conv91 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conc91 = concatenate([up9, conv91], axis=3)\n",
    "    conv92 = Conv2D(32, (3, 3), activation='relu', padding='same')(conc91)\n",
    "    conc92 = concatenate([up9, conv92], axis=3)\n",
    "    drop9 = Dropout(rate=dropout_rate)(conc92, training=True)\n",
    "\n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='sigmoid')(drop9)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def get_deep_unet(img_size=PATCH_SIZE, n_classes=N_CLASSES, n_input_channels=N_INPUT_CHANNELS, scale=1):\n",
    "    inputs = keras.Input(shape=img_size + (n_input_channels, ))\n",
    "\n",
    "    # Encoding Path of the UNet (32-64-128-256-512)\n",
    "    conv1   = Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(inputs)\n",
    "    drop1   = Dropout(rate=dropout_rate)(conv1, training=True)\n",
    "    max1    = MaxPooling2D((2, 2))(drop1)\n",
    "\n",
    "    conv2   = Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(max1)\n",
    "    drop2   = Dropout(rate=dropout_rate)(conv2, training=True)\n",
    "    max2    = MaxPooling2D((2, 2))(drop2)\n",
    "\n",
    "    conv3   = Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(max2)\n",
    "    drop3   = Dropout(rate=dropout_rate)(conv3, training=True)\n",
    "    max3    = MaxPooling2D((2, 2))(drop3)\n",
    "\n",
    "    conv4   = Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(max3)\n",
    "    drop4   = Dropout(rate=dropout_rate)(conv4, training=True)\n",
    "    max4    = MaxPooling2D((2, 2))(drop4)\n",
    "\n",
    "    lat     = Conv2D(512*scale, (3, 3), padding=\"same\", activation='relu')(max4)\n",
    "    drop5   = Dropout(rate=dropout_rate)(lat, training=True)\n",
    "\n",
    "    # Decoding Path of the UNet\n",
    "    up1     = UpSampling2D((2, 2))(drop5)\n",
    "    concat1 = concatenate([conv4, up1], axis=-1)\n",
    "    conv5   = Conv2D(256*scale, (3, 3), padding=\"same\", activation='relu')(concat1)\n",
    "    drop6   = Dropout(rate=dropout_rate)(conv5, training=True)\n",
    "    \n",
    "    up2     = UpSampling2D((2, 2))(drop6)\n",
    "    concat2 = concatenate([conv3, up2], axis=-1)\n",
    "    conv6   = Conv2D(128*scale, (3, 3), padding=\"same\", activation='relu')(concat2)\n",
    "    drop7   = Dropout(rate=dropout_rate)(conv6, training=True)\n",
    "    \n",
    "    up3     = UpSampling2D((2, 2))(drop7)\n",
    "    concat3 = concatenate([conv2, up3], axis=-1)\n",
    "    conv7   = Conv2D(64*scale, (3, 3), padding=\"same\", activation='relu')(concat3)\n",
    "    drop8   = Dropout(rate=dropout_rate)(conv7, training=True)\n",
    "\n",
    "    up4     = UpSampling2D((2, 2))(drop8)\n",
    "    concat4 = concatenate([conv1, up4], axis=-1)\n",
    "    conv8   = Conv2D(32*scale, (3, 3), padding=\"same\", activation='relu')(concat4)\n",
    "    drop9   = Dropout(rate=dropout_rate)(conv8, training=True)\n",
    "    \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation=\"softmax\")(drop9)\n",
    "\n",
    "    model   = Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByBoNkPGWKrN"
   },
   "source": [
    "# Generate Bias Corrected Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1641467747539,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "M9uR3S5BP_NO",
    "outputId": "9ba41734-d7b3-4909-bea0-8e262082dbb7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef N4(inputImagePath, maskImagePath, outputPath):\\n  # inputImagePath = input(\\'Enter the path of the image : \\')\\n  inputImage = sitk.ReadImage(inputImagePath)\\n\\n  print(\"N4 bias correction runs.\")\\n\\n  # maskImage = sitk.ReadImage(\"06-t1c_mask.nii.gz\")\\n  maskImage = sitk.OtsuThreshold(inputImage,0,1,200)\\n  maskImagePath = input(\\'Enter the name of the mask image to be saved : \\')\\n  sitk.WriteImage(maskImage, maskImagePath)\\n  print(\"Mask image is saved.\")\\n\\n  inputImage = sitk.Cast(inputImage,sitk.sitkFloat32)\\n\\n  corrector = sitk.N4BiasFieldCorrectionImageFilter();\\n\\n  output = corrector.Execute(inputImage,maskImage)\\n\\n  outputPath = input(\"Enter the name of the Bias Field Corrected Image :\")\\n  sitk.WriteImage(output,outputPath)\\n  print(\"Finished N4 Bias Field Correction.....\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def N4(inputImagePath, maskImagePath, outputPath):\n",
    "  # inputImagePath = input('Enter the path of the image : ')\n",
    "  inputImage = sitk.ReadImage(inputImagePath)\n",
    "\n",
    "  print(\"N4 bias correction runs.\")\n",
    "\n",
    "  # maskImage = sitk.ReadImage(\"06-t1c_mask.nii.gz\")\n",
    "  maskImage = sitk.OtsuThreshold(inputImage,0,1,200)\n",
    "  maskImagePath = input('Enter the name of the mask image to be saved : ')\n",
    "  sitk.WriteImage(maskImage, maskImagePath)\n",
    "  print(\"Mask image is saved.\")\n",
    "\n",
    "  inputImage = sitk.Cast(inputImage,sitk.sitkFloat32)\n",
    "\n",
    "  corrector = sitk.N4BiasFieldCorrectionImageFilter();\n",
    "\n",
    "  output = corrector.Execute(inputImage,maskImage)\n",
    "\n",
    "  outputPath = input(\"Enter the name of the Bias Field Corrected Image :\")\n",
    "  sitk.WriteImage(output,outputPath)\n",
    "  print(\"Finished N4 Bias Field Correction.....\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7e5tn28UYaO6"
   },
   "outputs": [],
   "source": [
    "def data_bias_correction(setName) :\n",
    "  \n",
    "  data_file = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/*'.format(setName)\n",
    "\n",
    "  for filename in glob.glob(data_file):\n",
    "    \n",
    "    #print(filename)\n",
    "    name = filename[-7:]\n",
    "    #print(name)\n",
    "    print(\"Working on image {0}\".format(name))\n",
    "\n",
    "    img_path = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}.nii.gz'.format(setName, name, name)\n",
    "    inputImage = sitk.ReadImage(img_path)\n",
    "    \n",
    "    mask_path = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_mask.nii.gz'.format(setName, name, name)\n",
    "    maskImage = sitk.OtsuThreshold(inputImage,0,1,200)\n",
    "    sitk.WriteImage(maskImage, mask_path)\n",
    "    print(\"Mask image is saved.\")\n",
    "\n",
    "    inputImage = sitk.Cast(inputImage,sitk.sitkFloat32)\n",
    "    corrector = sitk.N4BiasFieldCorrectionImageFilter();\n",
    "    output = corrector.Execute(inputImage,maskImage)\n",
    "\n",
    "    bias_corrected_path = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_bias_corrected.nii.gz'.format(setName, name, name)\n",
    "    sitk.WriteImage(output,bias_corrected_path)\n",
    "    print(\"Finished N4 Bias Field Correction.....\")\n",
    "\n",
    "    #bias_corrected_path = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_bias_corrected.nii.gz'.format(setName, name, name)\n",
    "    #N4(img_path, mask_path, bias_corrected_path)\n",
    "    #N4('/content/drive/MyDrive/MISA/Normal Segmentations/data/Training_Set/IBSR_01/IBSR_01.nii.gz', '/content/hello/', '/content/hello/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "O2InxCFyek-_"
   },
   "outputs": [],
   "source": [
    "# # Calling the Bias Removal Function (N4)\n",
    "# data_bias_correction('Training_Set')\n",
    "# data_bias_correction('Validation_Set')\n",
    "# data_bias_correction('Test_Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ2BevUuIkGs"
   },
   "source": [
    "# Denoising the Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9wQy2mw7ETMt"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# def anisodiff3(stack,niter=1,kappa=50,gamma=0.1,step=(1.,1.,1.),option=1,ploton=False):\n",
    "\n",
    "#     # ...you could always diffuse each color channel independently if you\n",
    "#     # really want\n",
    "#     if stack.ndim == 4:\n",
    "#         warnings.warn(\"Only grayscale stacks allowed, converting to 3D matrix\")\n",
    "#         stack = stack.mean(3)\n",
    "\n",
    "#     # initialize output array\n",
    "#     stack = stack.astype('float32')\n",
    "#     stackout = stack.copy()\n",
    "\n",
    "#     # initialize some internal variables\n",
    "#     deltaS = np.zeros_like(stackout)\n",
    "#     deltaE = deltaS.copy()\n",
    "#     deltaD = deltaS.copy()\n",
    "#     NS = deltaS.copy()\n",
    "#     EW = deltaS.copy()\n",
    "#     UD = deltaS.copy()\n",
    "#     gS = np.ones_like(stackout)\n",
    "#     gE = gS.copy()\n",
    "#     gD = gS.copy()\n",
    "\n",
    "#     # create the plot figure, if requested\n",
    "#     if ploton:\n",
    "#         import pylab as pl\n",
    "#         from time import sleep\n",
    "\n",
    "#         showplane = stack.shape[0]//2\n",
    "\n",
    "#         fig = pl.figure(figsize=(20,5.5),num=\"Anisotropic diffusion\")\n",
    "#         ax1,ax2 = fig.add_subplot(1,2,1),fig.add_subplot(1,2,2)\n",
    "\n",
    "#         ax1.imshow(stack[showplane,...].squeeze(),interpolation='nearest')\n",
    "#         ih = ax2.imshow(stackout[showplane,...].squeeze(),interpolation='nearest',animated=True)\n",
    "#         ax1.set_title(\"Original stack (Z = %i)\" %showplane)\n",
    "#         ax2.set_title(\"Iteration 0\")\n",
    "\n",
    "#         fig.canvas.draw()\n",
    "\n",
    "#     for ii in range(niter):\n",
    "\n",
    "#         # calculate the diffs\n",
    "#         deltaD[:-1,: ,:  ] = np.diff(stackout,axis=0)\n",
    "#         deltaS[:  ,:-1,: ] = np.diff(stackout,axis=1)\n",
    "#         deltaE[:  ,: ,:-1] = np.diff(stackout,axis=2)\n",
    "\n",
    "#         # conduction gradients (only need to compute one per dim!)\n",
    "#         if option == 1:\n",
    "#             gD = np.exp(-(deltaD/kappa)**2.)/step[0]\n",
    "#             gS = np.exp(-(deltaS/kappa)**2.)/step[1]\n",
    "#             gE = np.exp(-(deltaE/kappa)**2.)/step[2]\n",
    "#         elif option == 2:\n",
    "#             gD = 1./(1.+(deltaD/kappa)**2.)/step[0]\n",
    "#             gS = 1./(1.+(deltaS/kappa)**2.)/step[1]\n",
    "#             gE = 1./(1.+(deltaE/kappa)**2.)/step[2]\n",
    "\n",
    "#         # update matrices\n",
    "#         D = gD*deltaD\n",
    "#         E = gE*deltaE\n",
    "#         S = gS*deltaS\n",
    "\n",
    "#         # subtract a copy that has been shifted 'Up/North/West' by one\n",
    "#         # pixel. don't as questions. just do it. trust me.\n",
    "#         UD[:] = D\n",
    "#         NS[:] = S\n",
    "#         EW[:] = E\n",
    "#         UD[1:,: ,: ] -= D[:-1,:  ,:  ]\n",
    "#         NS[: ,1:,: ] -= S[:  ,:-1,:  ]\n",
    "#         EW[: ,: ,1:] -= E[:  ,:  ,:-1]\n",
    "\n",
    "#         # update the image\n",
    "#         stackout += gamma*(UD+NS+EW)\n",
    "\n",
    "#         if ploton:\n",
    "#             iterstring = \"Iteration %i\" %(ii+1)\n",
    "#             ih.set_data(stackout[showplane,...].squeeze())\n",
    "#             ax2.set_title(iterstring)\n",
    "#             fig.canvas.draw()\n",
    "#             # sleep(0.01)\n",
    "\n",
    "#     return stackout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CjVgNZuKJhtH"
   },
   "outputs": [],
   "source": [
    "# def denoise_volumes(in_volumes) :\n",
    "\n",
    "#   n_loop = in_volumes.shape[0]\n",
    "\n",
    "#   out_volumes = np.zeros(in_volumes.shape)\n",
    "#   #print(out_volumes.shape)\n",
    "\n",
    "#   for i in range(0,n_loop,1):\n",
    "#     temp = in_volumes[i,:,:,:,:]\n",
    "#     temp = anisodiff3(temp,niter=10)\n",
    "#     temp = temp.reshape((*temp.shape, 1))\n",
    "#     out_volumes[i] = temp\n",
    "\n",
    "#     #print(temp.shape)\n",
    "\n",
    "#   return out_volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S9QIgQ--I6Ud"
   },
   "outputs": [],
   "source": [
    "# t_volumes = denoise_volumes(t_volumes)\n",
    "# v_volumes = denoise_volumes(v_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1641467765677,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "sfo3B9gWKRrY",
    "outputId": "50be977a-f6c6-44c5-ad9c-4c840315d3f6"
   },
   "outputs": [],
   "source": [
    "#t_volumes = t_volumes_clean\n",
    "#v_volumes = v_volumes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9Omu--2PGX6P"
   },
   "outputs": [],
   "source": [
    "# check_vol_clean = anisodiff3(check_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r-9Q9zKvGsdB"
   },
   "outputs": [],
   "source": [
    "# rotated_vol_clean = ndimage.rotate(check_vol_clean, 90)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(rotated_vol_clean[:, :, 150], cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z1E1IglE-0w"
   },
   "source": [
    "# Loading the training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cgujGm9tX_Rj"
   },
   "outputs": [],
   "source": [
    "def load_data_bias(image_size, setName) :\n",
    "\n",
    "    #data_file = '/content/drive/My Drive/MISA/Normal Segmentations/data/{}/*'.format(setName)\n",
    "    data_file = 'data/{}/*'.format(setName)\n",
    "\n",
    "    folders = glob.glob(data_file)\n",
    "    n_volumes = len(folders)\n",
    "\n",
    "    volumes = np.zeros((n_volumes, *image_size, 1))\n",
    "    labels = np.zeros((n_volumes, *image_size, 1))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for filename in glob.glob(data_file):\n",
    "\n",
    "        #print(filename)\n",
    "        name = filename[-7:]\n",
    "        #print(name)\n",
    "\n",
    "        #img_data = nib.load('/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_bias_corrected.nii.gz'.format(setName, name, name))\n",
    "        img_data = nib.load('data/{}/{}/{}_bias_corrected.nii.gz'.format(setName, name, name))\n",
    "        img_data_temp = img_data.get_fdata()\n",
    "        img_data_temp = img_data_temp.reshape((*image_size, 1))\n",
    "        #print(img_data_temp.shape)\n",
    "        volumes[i] = img_data_temp\n",
    "\n",
    "        #seg_data = nib.load('/content/drive/My Drive/MISA/Normal Segmentations/data/{}/{}/{}_seg.nii.gz'.format(setName, name, name))\n",
    "        seg_data = nib.load('data/{}/{}/{}_seg.nii.gz'.format(setName, name, name))\n",
    "        labels[i] = seg_data.get_fdata()\n",
    "\n",
    "        print(\"Working on image {0}\".format(name))\n",
    "        i = i+1\n",
    "        \n",
    "        if(setName == 'Validation_Set'):\n",
    "            list_id.append(name)\n",
    "\n",
    "    return (volumes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18144,
     "status": "ok",
     "timestamp": 1641467765669,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "IoSMUWMWgKQj",
    "outputId": "07ee3e8d-e80d-4ee9-e493-b185a51c3d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on image IBSR_18\n",
      "Working on image IBSR_16\n",
      "Working on image IBSR_05\n",
      "Working on image IBSR_03\n",
      "Working on image IBSR_04\n",
      "Working on image IBSR_08\n",
      "Working on image IBSR_01\n",
      "Working on image IBSR_06\n",
      "Working on image IBSR_07\n",
      "Working on image IBSR_09\n",
      "Working on image IBSR_17\n",
      "Working on image IBSR_13\n",
      "Working on image IBSR_14\n",
      "Working on image IBSR_12\n",
      "['IBSR_17', 'IBSR_13', 'IBSR_14', 'IBSR_12']\n"
     ]
    }
   ],
   "source": [
    "(t_volumes, t_labels) = load_data_bias(IMAGE_SIZE, 'Training_Set')\n",
    "(v_volumes, v_labels) = load_data_bias(IMAGE_SIZE, 'Validation_Set')\n",
    "\n",
    "print(list_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 128, 256, 1)\n",
      "(4, 256, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(t_volumes.shape)\n",
    "print(v_volumes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ3ZCG2Wc92_"
   },
   "source": [
    "Visualising the Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NuRrxxLM9TCp"
   },
   "outputs": [],
   "source": [
    "# check_vol = t_volumes[1,:,:,:,:]\n",
    "# check_vol = check_vol.reshape((256, 128, 256))\n",
    "# rotated_vol = ndimage.rotate(check_vol, 90)\n",
    "# plt.axis('off')\n",
    "# plt.imshow(rotated_vol[:, :, 150], cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAok6QlCeBm0"
   },
   "source": [
    "Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1641467765992,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "x_aGLgc60ceM",
    "outputId": "87ea3218-9565-4b44-e81a-ede12dc19f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 128, 256, 1)\n",
      "(4, 256, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into training and validation\n",
    "#training_volumes = t_volumes[TRAINING_VOLUMES]\n",
    "#training_labels = t_labels[TRAINING_VOLUMES]\n",
    "training_volumes = t_volumes\n",
    "training_labels = t_labels\n",
    "\n",
    "#validation_volumes = t_volumes[VALIDATION_VOLUMES]\n",
    "#validation_labels = t_labels[VALIDATION_VOLUMES]\n",
    "validation_volumes = v_volumes\n",
    "validation_labels = v_labels\n",
    "\n",
    "print(training_volumes.shape)\n",
    "#print(training_labels.shape)\n",
    "\n",
    "print(validation_volumes.shape)\n",
    "#print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFDtzyW6FH7x"
   },
   "source": [
    "# **Extract *useful* patches**\n",
    "\n",
    "This step is fundamental, we want to provide the network with useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vMvPbAZ4gb8C"
   },
   "outputs": [],
   "source": [
    "def extract_patches(x, patch_size, patch_stride) :\n",
    "  return tf.image.extract_patches(\n",
    "    x,\n",
    "    sizes=[1, *patch_size, 1],\n",
    "    strides=[1, *patch_stride, 1],\n",
    "    rates=[1, 1, 1, 1],\n",
    "    padding='SAME', name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nsKNTm-Lf-sb"
   },
   "outputs": [],
   "source": [
    "def extract_useful_patches(\n",
    "    volumes, labels,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    stride=PATCH_STRIDE,\n",
    "    threshold=CONTENT_THRESHOLD,\n",
    "    num_classes=N_CLASSES) :\n",
    "\n",
    "    volumes = volumes.reshape([-1, image_size[1], image_size[2], 1])\n",
    "    labels = labels.reshape([-1, image_size[1], image_size[2], 1])\n",
    "\n",
    "    vol_patches = extract_patches(volumes, patch_size, stride).numpy()\n",
    "    seg_patches = extract_patches(labels, patch_size, stride).numpy()\n",
    "\n",
    "    vol_patches = vol_patches.reshape([-1, *patch_size, 1])\n",
    "    seg_patches = seg_patches.reshape([-1, *patch_size, ])\n",
    "\n",
    "    # this will get rid of the background and only take foreground\n",
    "    foreground_mask = seg_patches != 0 \n",
    "\n",
    "    # we only keep the useful forground patches\n",
    "    # threshold too small - takes even the useless patches\n",
    "    # threshold too high - might leave out useful patches\n",
    "    useful_patches = foreground_mask.sum(axis=(1, 2)) > threshold * np.prod(patch_size)\n",
    "\n",
    "    vol_patches = vol_patches[useful_patches]\n",
    "    seg_patches = seg_patches[useful_patches]\n",
    "\n",
    "    seg_patches = tf.keras.utils.to_categorical(\n",
    "    seg_patches, num_classes=N_CLASSES, dtype='float32')\n",
    "\n",
    "    return (vol_patches, seg_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2002,
     "status": "ok",
     "timestamp": 1641467770458,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "tTcoEi426bfg",
    "outputId": "786a957e-2dbc-4229-9e07-922b842678f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 18:28:19.215573: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-01-16 18:28:19.216209: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "(12747, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# extract patches from training set\n",
    "(training_patches, training_patches_seg) = extract_useful_patches(training_volumes, training_labels)\n",
    "\n",
    "# extract patches from validation set\n",
    "(validation_patches, validation_patches_seg) = extract_useful_patches(validation_volumes, validation_labels)\n",
    "\n",
    "print(training_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"from random import randrange\n",
    "\n",
    "index_to_view = randrange(12747)\n",
    "check_vol = training_patches[index_to_view,:,:,:]\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(check_vol, cmap='gray')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mx9fsloJgxrm"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Bej2lLFyedQ_"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        #rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        #data_format = \"channels_last\",\n",
    "        fill_mode='nearest') #reflect, wrap, constant(black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6o21TRkXedUO"
   },
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(training_patches, batch_size=int(training_patches.shape[0]/BATCH_SIZE), seed=1)\n",
    "train_label_generator = datagen.flow(training_patches_seg, batch_size=int(training_patches.shape[0]/BATCH_SIZE), seed=1)\n",
    "\n",
    "val_generator = datagen.flow(validation_patches, batch_size=int(validation_patches.shape[0]/BATCH_SIZE), seed=1)\n",
    "val_label_generator = datagen.flow(validation_patches_seg, batch_size=int(validation_patches.shape[0]/BATCH_SIZE), seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "sOw8_p4Oedag"
   },
   "outputs": [],
   "source": [
    "X_train = train_generator.next()\n",
    "y_train = train_label_generator.next()\n",
    "\n",
    "X_val = val_generator.next()\n",
    "y_val = val_label_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1641467770785,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "IELRGzM3r2mz",
    "outputId": "975ff501-07ef-45ea-f718-f99273345b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12747, 32, 32, 1)\n",
      "(12747, 32, 32, 4)\n",
      "----------------\n",
      "(5585, 32, 32, 1)\n",
      "(5585, 32, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "print(training_patches.shape)\n",
    "print(training_patches_seg.shape)\n",
    "print(\"----------------\")\n",
    "print(validation_patches.shape)\n",
    "print(validation_patches_seg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1641467770786,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "ce8mcx6Nr3QG",
    "outputId": "c23d258a-b6d1-46d3-acb6-6c4c3967e229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12946, 32, 32, 1)\n",
      "(12946, 32, 32, 4)\n",
      "(5672, 32, 32, 1)\n",
      "(5672, 32, 32, 4)\n"
     ]
    }
   ],
   "source": [
    "full_train = np.concatenate((training_patches, X_train))\n",
    "print(full_train.shape)\n",
    "full_train_label = np.concatenate((training_patches_seg, y_train))\n",
    "print(full_train_label.shape)\n",
    "\n",
    "full_val = np.concatenate((validation_patches, X_val))\n",
    "print(full_val.shape)\n",
    "full_val_label = np.concatenate((validation_patches_seg, y_val))\n",
    "print(full_val_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fdqvKEK1UES"
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJYxf3HiFopr"
   },
   "source": [
    "**Instantiate UNet model and train it**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkBp70aWGB_b"
   },
   "source": [
    "Using callbacks to stop training and avoid overfitting\n",
    "\n",
    "\n",
    "*   Early stopping with a certain patience\n",
    "*   Save (and load!) best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 162471,
     "status": "ok",
     "timestamp": 1641467975906,
     "user": {
      "displayName": "Sheikh Adilina",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11368981208219818714"
     },
     "user_tz": -60
    },
    "id": "j7H9hIPwEwJZ",
    "outputId": "6c0cea8e-182a-42db-9005-6f21fd6b8ad1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 22:34:55.945469: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-13 22:34:55.945675: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 22:34:56.158473: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - ETA: 0s - loss: 1.1122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 22:35:07.015792: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 13s 64ms/step - loss: 1.1122 - val_loss: 0.5668\n",
      "Epoch 2/200\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.5073 - val_loss: 0.4584\n",
      "Epoch 3/200\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.4260 - val_loss: 0.3978\n",
      "Epoch 4/200\n",
      "203/203 [==============================] - 13s 65ms/step - loss: 0.3729 - val_loss: 0.3739\n",
      "Epoch 5/200\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.3388 - val_loss: 0.3846\n",
      "Epoch 6/200\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.3247 - val_loss: 0.3214\n",
      "Epoch 7/200\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.3001 - val_loss: 0.3164\n",
      "Epoch 8/200\n",
      "203/203 [==============================] - 13s 63ms/step - loss: 0.2841 - val_loss: 0.2955\n",
      "Epoch 9/200\n",
      "203/203 [==============================] - 13s 63ms/step - loss: 0.2641 - val_loss: 0.2832\n",
      "Epoch 10/200\n",
      "203/203 [==============================] - 13s 64ms/step - loss: 0.2535 - val_loss: 0.2732\n",
      "Epoch 11/200\n",
      "203/203 [==============================] - 13s 62ms/step - loss: 0.2429 - val_loss: 0.2975\n",
      "Epoch 12/200\n",
      "203/203 [==============================] - 13s 63ms/step - loss: 0.2395 - val_loss: 0.2706\n",
      "Epoch 13/200\n",
      "203/203 [==============================] - 13s 63ms/step - loss: 0.2309 - val_loss: 0.2608\n",
      "Epoch 14/200\n",
      "203/203 [==============================] - 13s 63ms/step - loss: 0.2232 - val_loss: 0.2561\n",
      "Epoch 15/200\n",
      "203/203 [==============================] - 13s 62ms/step - loss: 0.2205 - val_loss: 0.2841\n",
      "Epoch 16/200\n",
      "203/203 [==============================] - 13s 63ms/step - loss: 0.2141 - val_loss: 0.2510\n",
      "Epoch 17/200\n",
      "203/203 [==============================] - 14s 68ms/step - loss: 0.2117 - val_loss: 0.2531\n",
      "Epoch 18/200\n",
      "203/203 [==============================] - 13s 62ms/step - loss: 0.2073 - val_loss: 0.2457\n",
      "Epoch 19/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.2025 - val_loss: 0.2426\n",
      "Epoch 20/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1973 - val_loss: 0.2457\n",
      "Epoch 21/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1950 - val_loss: 0.2373\n",
      "Epoch 22/200\n",
      "203/203 [==============================] - 13s 62ms/step - loss: 0.1905 - val_loss: 0.2365\n",
      "Epoch 23/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1869 - val_loss: 0.2322\n",
      "Epoch 24/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1852 - val_loss: 0.2318\n",
      "Epoch 25/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1835 - val_loss: 0.2372\n",
      "Epoch 26/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1822 - val_loss: 0.2353\n",
      "Epoch 27/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1831 - val_loss: 0.2481\n",
      "Epoch 28/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1780 - val_loss: 0.2317\n",
      "Epoch 29/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1758 - val_loss: 0.2306\n",
      "Epoch 30/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1754 - val_loss: 0.2316\n",
      "Epoch 31/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1730 - val_loss: 0.2324\n",
      "Epoch 32/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1687 - val_loss: 0.2281\n",
      "Epoch 33/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1686 - val_loss: 0.2321\n",
      "Epoch 34/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1689 - val_loss: 0.2365\n",
      "Epoch 35/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1684 - val_loss: 0.2475\n",
      "Epoch 36/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1644 - val_loss: 0.2555\n",
      "Epoch 37/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1645 - val_loss: 0.2222\n",
      "Epoch 38/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1623 - val_loss: 0.2289\n",
      "Epoch 39/200\n",
      "203/203 [==============================] - 12s 60ms/step - loss: 0.1617 - val_loss: 0.2240\n",
      "Epoch 40/200\n",
      "203/203 [==============================] - 12s 61ms/step - loss: 0.1587 - val_loss: 0.2239\n",
      "Epoch 41/200\n",
      "203/203 [==============================] - 12s 62ms/step - loss: 0.1580 - val_loss: 0.2279\n",
      "Epoch 42/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1579 - val_loss: 0.2208\n",
      "Epoch 43/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1584 - val_loss: 0.2197\n",
      "Epoch 44/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1551 - val_loss: 0.2469\n",
      "Epoch 45/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1557 - val_loss: 0.2233\n",
      "Epoch 46/200\n",
      "203/203 [==============================] - 12s 57ms/step - loss: 0.1541 - val_loss: 0.2382\n",
      "Epoch 47/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1534 - val_loss: 0.2192\n",
      "Epoch 48/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1511 - val_loss: 0.2343\n",
      "Epoch 49/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1518 - val_loss: 0.2296\n",
      "Epoch 50/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1500 - val_loss: 0.2299\n",
      "Epoch 51/200\n",
      "203/203 [==============================] - 12s 57ms/step - loss: 0.1483 - val_loss: 0.2143\n",
      "Epoch 52/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1493 - val_loss: 0.2404\n",
      "Epoch 53/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1479 - val_loss: 0.2274\n",
      "Epoch 54/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1471 - val_loss: 0.2289\n",
      "Epoch 55/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1455 - val_loss: 0.2253\n",
      "Epoch 56/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1459 - val_loss: 0.2261\n",
      "Epoch 57/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1467 - val_loss: 0.2145\n",
      "Epoch 58/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1436 - val_loss: 0.2234\n",
      "Epoch 59/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1432 - val_loss: 0.2172\n",
      "Epoch 60/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1430 - val_loss: 0.2204\n",
      "Epoch 61/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1430 - val_loss: 0.2294\n",
      "Epoch 62/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1414 - val_loss: 0.2187\n",
      "Epoch 63/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1421 - val_loss: 0.2298\n",
      "Epoch 64/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1404 - val_loss: 0.2312\n",
      "Epoch 65/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1411 - val_loss: 0.2248\n",
      "Epoch 66/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1418 - val_loss: 0.2434\n",
      "Epoch 67/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1399 - val_loss: 0.2205\n",
      "Epoch 68/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1404 - val_loss: 0.2257\n",
      "Epoch 69/200\n",
      "203/203 [==============================] - 12s 59ms/step - loss: 0.1387 - val_loss: 0.2247\n",
      "Epoch 70/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1389 - val_loss: 0.2436\n",
      "Epoch 71/200\n",
      "203/203 [==============================] - 12s 58ms/step - loss: 0.1375 - val_loss: 0.2189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3877fb0a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=PATIENCE), # early stopping\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=MODEL_FNAME_PATTERN, save_best_only=True) # save the best based on validation\n",
    "]\n",
    "\n",
    "unet = get_unet()\n",
    "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
    "unet.fit(\n",
    "    x=full_train, \n",
    "    y=full_train_label,\n",
    "    validation_data=(full_val, full_val_label),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=N_EPOCHS,\n",
    "    callbacks=my_callbacks,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGcfrZCMjz4M"
   },
   "source": [
    "# Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5fNsYJL7KnON"
   },
   "outputs": [],
   "source": [
    "unet = get_unet(\n",
    "    img_size=(IMAGE_SIZE[1], IMAGE_SIZE[2]),\n",
    "    n_classes=N_CLASSES,\n",
    "    n_input_channels=N_INPUT_CHANNELS)\n",
    "unet.compile(optimizer=OPTIMISER, loss=LOSS)\n",
    "unet.load_weights('model-200-categorical_crossentropy.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwPzN0jrSm8r"
   },
   "source": [
    "##Prepare test data using the validation volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vuRHLNYGkDFp"
   },
   "outputs": [],
   "source": [
    "def prepare_val_data(the_volumes, the_labels):\n",
    "\n",
    "    testing_volumes_processed = the_volumes.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
    "    testing_labels_processed = the_labels.reshape([-1, IMAGE_SIZE[1], IMAGE_SIZE[2], 1])\n",
    "\n",
    "    testing_labels_processed = tf.keras.utils.to_categorical(testing_labels_processed, num_classes=4, dtype='float32')\n",
    "\n",
    "    #print(testing_volumes_processed.shape)\n",
    "    #print(testing_labels_processed.shape)\n",
    "\n",
    "    return (testing_volumes_processed, testing_labels_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbJZUQyLqF4p"
   },
   "source": [
    "###Predict labels for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ItoY31x0K3r8"
   },
   "outputs": [],
   "source": [
    "def pred_val_data(testing_volumes_processed)  :\n",
    "\n",
    "    # creates probability map of each label for all volumes\n",
    "    prediction = unet.predict(x=testing_volumes_processed)\n",
    "\n",
    "    prediction = np.argmax(prediction, axis=3)\n",
    "\n",
    "    #plt.axis('off')\n",
    "    #plt.imshow(prediction[:, :, 150])\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RHaF_LhpWXr"
   },
   "source": [
    "##Computing Dice, AVD and HD for the validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iFBnx17Bpa_b"
   },
   "outputs": [],
   "source": [
    "def compute_hausdorff_distance(in1, in2, label):\n",
    "    in1=in1.squeeze()\n",
    "    in2=in2.squeeze()\n",
    "    hausdorff_distance_filter = sitk.HausdorffDistanceImageFilter()\n",
    "    if label == 'all':\n",
    "        # Hausdorff distance\n",
    "        hausdorff_distance_filter.Execute(in1, in2)\n",
    "    else:\n",
    "        in1_array  = in1 #sitk.GetArrayFromImage(in1)\n",
    "        in1_array = (in1_array == label) *1 \n",
    "        in1_array = in1_array.astype('uint16')  \n",
    "        img1 = sitk.GetImageFromArray(in1_array)\n",
    "        \n",
    "        in2_array  = in2 #sitk.GetArrayFromImage(in2)\n",
    "        in2_array = (in2_array == label) *1 \n",
    "        in2_array = in2_array.astype('uint16')  \n",
    "        img2 = sitk.GetImageFromArray(in2_array)\n",
    "        # Hausdorff distance\n",
    "        hausdorff_distance_filter.Execute(img1, img2)\n",
    "    return hausdorff_distance_filter.GetHausdorffDistance()\n",
    "\n",
    "def compute_dice_coefficient(in1, in2, label):\n",
    "    in1=in1.squeeze()\n",
    "    in2=in2.squeeze()\n",
    "    if label=='all': \n",
    "        return 2 * np.sum( (in1>0) &  (in2>0) & (in1 == in2)) / (np.sum(in1 > 0) + np.sum(in2 > 0))\n",
    "    else:\n",
    "        return 2 * np.sum((in1 == label) & (in2 == label)) / (np.sum(in1 == label) + np.sum(in2 == label))\n",
    "\n",
    "def compute_volumentric_difference(in1, in2, label):\n",
    "    in1=in1.squeeze()\n",
    "    in2=in2.squeeze()\n",
    "    if label  == 'all':\n",
    "      #  vol_dif  = np.sum((in1 != in2) & (in1 !=0) & (in2 !=0))\n",
    "        return np.sum((in1 != in2)) / ((np.sum(in1 > 0) + np.sum(in2 > 0)))\n",
    "    else:\n",
    "        in1  = (in1 == label) * 1\n",
    "        in2  = (in2 == label) * 1\n",
    "        return np.sum((in1 != in2)) / ((np.sum(in1 > 0) + np.sum(in2 > 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8PrStjpepejb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 22:49:30.006304: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0\n",
      "\tDice Coefficient = 0.9968\n",
      "\tHD = 20.7774\n",
      "\tAVD = 0.0032\n",
      "Class 1\n",
      "\tDice Coefficient = 0.8220\n",
      "\tHD = 136.1081\n",
      "\tAVD = 0.1780\n",
      "Class 2\n",
      "\tDice Coefficient = 0.9163\n",
      "\tHD = 87.7016\n",
      "\tAVD = 0.0837\n",
      "Class 3\n",
      "\tDice Coefficient = 0.8814\n",
      "\tHD = 121.2623\n",
      "\tAVD = 0.1186\n"
     ]
    }
   ],
   "source": [
    "for cl in range(0,4,1): \n",
    "    \n",
    "    overallDSC = np.zeros(N_CLASSES)\n",
    "    overall_Hausdorff = np.zeros(N_CLASSES)\n",
    "    overall_vol = np.zeros(N_CLASSES)\n",
    "    \n",
    "\n",
    "    for i in range(0,validation_volumes.shape[0], 1):\n",
    "\n",
    "        testing_volumes_processed, testing_labels_processed = prepare_val_data(v_volumes[i], v_labels[i])\n",
    "        prediction = pred_val_data(testing_volumes_processed)\n",
    "        \n",
    "        if (cl == 0):\n",
    "            \n",
    "            name = list_id[i]\n",
    "            temp_img = nib.load('data/Validation_Set/{}/{}.nii.gz'.format(name, name))\n",
    "\n",
    "            image_to_save = nib.Nifti1Image(prediction, temp_img.affine)\n",
    "            image_to_save.set_data_dtype(np.uint16)\n",
    "\n",
    "            #print(image_to_save.shape)\n",
    "            nib.save(image_to_save, 'our_code_validated_seg/our_{0}.nii.gz'.format(name))\n",
    "        #cl = 3\n",
    "        \n",
    "        cur_DSC = compute_dice_coefficient(prediction, v_labels[i], label=cl)\n",
    "        overallDSC = overallDSC + cur_DSC\n",
    "\n",
    "        cur_Hausdorff = compute_hausdorff_distance(prediction, v_labels[i], label=cl) \n",
    "        overall_Hausdorff = overall_Hausdorff + cur_Hausdorff\n",
    "\n",
    "        cur_vol = compute_volumentric_difference(prediction, v_labels[i], label=cl)\n",
    "        overall_vol = overall_vol + cur_vol\n",
    "\n",
    "        #print(prediction.shape)\n",
    "        #print(v_labels[i].shape)\n",
    "\n",
    "    #print(overall_Hausdorff)\n",
    "    overallDSC = overallDSC/validation_volumes.shape[0]\n",
    "    overall_Hausdorff = overall_Hausdorff/validation_volumes.shape[0]\n",
    "    overall_vol = overall_vol/validation_volumes.shape[0]\n",
    "\n",
    "    print(\"Class {}\".format(cl))\n",
    "    print(\"\\tDice Coefficient = {:.4f}\".format(overallDSC[i]))\n",
    "    print(\"\\tHD = {:.4f}\".format(overall_Hausdorff[i]))\n",
    "    print(\"\\tAVD = {:.4f}\".format(overall_vol[i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HMIG5gOwa-Nb",
    "IEFUu8FjYvaz",
    "kakQO5X9E0oR",
    "ByBoNkPGWKrN",
    "4Z1E1IglE-0w",
    "uZ2BevUuIkGs",
    "LpKWGJ-Fh1B_",
    "mx9fsloJgxrm",
    "XGcfrZCMjz4M",
    "CwPzN0jrSm8r"
   ],
   "name": "Bias_Corrected_Patch_Augment_2DUNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MISA course",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
